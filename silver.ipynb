{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37613c51-f50f-4c5f-a778-ef6c1f7e5cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_c = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"abfs://bronze@storagekevinav.dfs.core.windows.net/sales_view/customer/\")\n",
    "display(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76902a2-310f-4f2f-847e-dd913a63b0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CamelToSnake_Example\").getOrCreate()\n",
    "\n",
    "\n",
    "def camel_to_snake(name: str) -> str:\n",
    "    \n",
    "    s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "def convert_columns_to_snakecase(df: DataFrame) -> DataFrame:\n",
    "    new_cols = [camel_to_snake(c) for c in df_c.columns]\n",
    "    return df_c.toDF(*new_cols)\n",
    "\n",
    "camel_to_snake_udf = udf(lambda x: camel_to_snake(x) if x is not None else None, StringType())\n",
    "spark.udf.register(\"camel_to_snake_udf\", camel_to_snake_udf)\n",
    "df = spark.read.option(\"header\", \"true\").csv(\n",
    "    \"abfss://bronze@storagekevinav.dfs.core.windows.net/sales_view/customer/\"\n",
    ")\n",
    "for c in df.columns: df = df_c.withColumn(c, camel_to_snake_udf(col(c)))\n",
    "\n",
    "print(\" Columns after rename:\", df_c.columns)\n",
    "display(df_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2c4f67-ca44-4874-a10a-906305cc12a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,split\n",
    "df_split2 = df_c.withColumn(\"first_name\", split(col(\"Name\"), \" \").getItem(0)) \\\n",
    "             .withColumn(\"last_name\", split(col(\"Name\"), \" \").getItem(1))\n",
    "display(df_split2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57207bf-92ab-4751-a521-5e886bfb7dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "df_with_domain = df_split2.withColumn(\n",
    "    \"domain\",\n",
    "    split(split(col(\"Email Id\"), \"@\").getItem(1), \"\\.\").getItem(0)\n",
    "    \n",
    ")\n",
    "display(df_with_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f27d8f1-4359-4c27-b451-40dacb3de2f8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763019173680}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lower, trim\n",
    "\n",
    "df4 = df_with_domain.withColumn(\n",
    "    \"gender\",\n",
    "    when(lower(trim(col(\"gender\"))) == \"female\", \"F\")\n",
    "    .when(lower(trim(col(\"gender\"))) == \"male\", \"M\")\n",
    "    .otherwise(col(\"gender\"))\n",
    ")\n",
    "\n",
    "display(df4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b306356d-158d-4f2c-9201-4936cad54750",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763022209296}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, to_date, split\n",
    " \n",
    "df_dateAndTime = (\n",
    "   df4\n",
    "        # STEP 1: Convert entire string to a proper timestamp\n",
    "        .withColumn(\"ts\", to_timestamp(col(\"Joining Date\"), \"dd-MM-yyyy HH:mm\"))\n",
    "       \n",
    "        # STEP 2: Extract date in yyyy-MM-dd format\n",
    "        .withColumn(\"date\", to_date(col(\"ts\")))\n",
    "       \n",
    "        # STEP 3: Extract time as HH:mm:ss\n",
    "        .withColumn(\"time\", split(col(\"joining Date\"), \" \").getItem(1))\n",
    " \n",
    "        # drop extra column\n",
    "        .drop(\"ts\")\n",
    ")\n",
    " \n",
    "display(df_dateAndTime)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "670bd6ff-49a9-4436-926c-4b2d1e155356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Derive “expenditure_status” based on “spent” column\n",
    "df_expenditureStatus = df4.withColumn(\n",
    "    \"expenditure_status\",\n",
    "    when(col(\"spent\") < 500, \"Minimum\").otherwise(\"Maximum\")\n",
    ")\n",
    "display(df_expenditureStatus)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11521e7b-2e3b-4c80-83cf-ac9d4e93d6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    new_cols = []\n",
    "    for c in df_expenditureStatus.columns:\n",
    "        # Remove invalid chars:  ,;{}()\\n\\t=\n",
    "        clean = re.sub(r\"[ ,;{}()\\n\\t=]\", \"_\", c)\n",
    "        # Replace multiple underscores with one\n",
    "        clean = re.sub(\"_+\", \"_\", clean)\n",
    "        # Convert to lowercase\n",
    "        clean = clean.lower()\n",
    "        new_cols.append(clean)\n",
    "    return df_expenditureStatus.toDF(*new_cols)\n",
    " \n",
    "df_clean = clean_column_names(df_expenditureStatus)\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82a2ceb-e1c7-4367-a7f3-e3221a60d2d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_unique_c = df_clean.dropDuplicates([\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a0b0d15-68f6-455c-97a6-605272c070c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "table_name = \"silver_data.sales_view.customer\"\n",
    " \n",
    "from delta.tables import DeltaTable\n",
    " \n",
    "if spark.catalog.tableExists(table_name):\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "   \n",
    "    (delta_table.alias(\"target\")\n",
    "     .merge(\n",
    "         df_unique_c.alias(\"source\"),\n",
    "         \"target.customer_id = source.customer_id\"\n",
    "     )\n",
    "     .whenMatchedUpdateAll()\n",
    "     .whenNotMatchedInsertAll()\n",
    "     .execute()\n",
    "    )\n",
    " \n",
    "else:\n",
    "    (df_unique_c.write.format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .saveAsTable(table_name))\n",
    " \n",
    "    print(\" Product data successfully upserted into Silver layer Catalog.\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9659a7-7d44-4a57-8558-8f5cbb27cd21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(silver_path).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a211b0d-e06e-4c96-baaa-b0e127a34c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Product Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e854de-efd5-4f7f-a91d-bd4270a8b85a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"abfss://bronze@storagekevinav.dfs.core.windows.net/sales_view/product/\",inferSchema=True,header=True)\n",
    "display(df)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a30d32e-94a7-4317-9117-813d27644e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "def rename_columns_to_snake_case(df):\n",
    "    def camel_to_snake(colname):\n",
    "        s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', colname)\n",
    "        colname = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "        return colname.lower().replace(\" \", \"_\")\n",
    "   \n",
    "    new_cols = [camel_to_snake(c) for c in df.columns]\n",
    "    return df.toDF(*new_cols)\n",
    " \n",
    "df_snake_p = rename_columns_to_snake_case(df)\n",
    "df_snake_p.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1452ca4a-716f-4c33-886e-77ef2b469962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    " \n",
    "df_create = df_snake_p.withColumn(\n",
    "    \"sub_category\",\n",
    "    when(col(\"category_id\") == 1, \"phone\")\n",
    "    .when(col(\"category_id\") == 2, \"laptop\")\n",
    "    .when(col(\"category_id\") == 3, \"playstation\")\n",
    "    .when(col(\"category_id\") == 4, \"e-device\")\n",
    "    .otherwise(\"unknown\")\n",
    ")\n",
    "display(df_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf261b2-3e4e-426b-8235-015a97c99fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dup = df_create.dropDuplicates([\"product_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b6d8f4-c1f5-44aa-8e9c-07acc1007ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "#spark.sql(\"DESCRIBE TABLE assignment_adf.sales_view.product\").show(truncate=False)\n",
    "\n",
    "#spark.sql(\"drop table if exists assignment_adf.sales_view.product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c47bec1-5a4c-4a0e-a3f9-c8d24ef6d10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from delta.tables import DeltaTable\n",
    " \n",
    "table_name = \"silver_data.sales_view.product\"\n",
    " \n",
    "if spark.catalog.tableExists(table_name):\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "   \n",
    "    (delta_table.alias(\"target\")\n",
    "     .merge(\n",
    "         df_dup.alias(\"source\"),\n",
    "         \"target.product_id = source.product_id\"\n",
    "     )\n",
    "     .whenMatchedUpdateAll()\n",
    "     .whenNotMatchedInsertAll()\n",
    "     .execute()\n",
    "    )\n",
    " \n",
    "else:\n",
    "    (df_dup.write.format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .saveAsTable(table_name))\n",
    " \n",
    "    print(\" Product data successfully upserted into Silver layer Catalog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac991b57-7621-416f-b799-47fa1b1da67d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763101055794}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"abfs://bronze@storagekevinav.dfs.core.windows.net/sales_view/stote/\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71e3478-0bbc-4c5f-844b-b929ebebf92b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763135445971}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns_to_snake_case(df):\n",
    "    def camel_to_snake(colname):\n",
    "        s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', colname)\n",
    "        colname = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "        return colname.lower().replace(\" \", \"_\")\n",
    "   \n",
    "    new_cols = [camel_to_snake(c) for c in df.columns]\n",
    "    return df.toDF(*new_cols)\n",
    " \n",
    "df_store = rename_columns_to_snake_case(df)\n",
    "display(df_store)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc84ab2-5ddf-41ee-ae0d-dfe1dba239fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "df_2 = df_store.withColumn(\n",
    "    \"store_category\",\n",
    "    regexp_extract(col(\"email_address\"), \"@(.*?)\\\\.\", 1)\n",
    ")\n",
    "display(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9066fe1c-6b55-4c74-8583-fbbdf4544420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, date_format, current_date\n",
    "df_store_final = (\n",
    "    df_2\n",
    "    .withColumn(\"created_at\", date_format(current_date(), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"updated_at\", date_format(current_date(), \"yyyy-MM-dd\"))\n",
    ")\n",
    "\n",
    "display(df_store_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb8a6fe-7302-44cf-a3ad-4f23976baa37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dup = df_store_final.dropDuplicates([\"store_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8a81bc-c199-4e84-8f64-3ec34b0625d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    " \n",
    "\n",
    "table_name  = \"silver_data.sales_view.store\"\n",
    " \n",
    "if spark.catalog.tableExists(table_name):\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    " \n",
    "    (\n",
    "\n",
    "        delta_table.alias(\"target\")\n",
    "\n",
    "            .merge(\n",
    "\n",
    "                df_dup.alias(\"source\"),\n",
    "\n",
    "                \"target.store_id = source.store_id\"\n",
    "\n",
    "            )\n",
    "\n",
    "            .whenMatchedUpdateAll()\n",
    "\n",
    "            .whenNotMatchedInsertAll()\n",
    "\n",
    "            .execute()\n",
    "\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "\n",
    "    df_dup.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15576e46-0000-4f77-ae4a-ff230c74a1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e2216fc-d2af-4a49-84d3-0c0381161846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sales Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c485c66d-5a93-44e4-981a-bce8fd97df1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfsales=spark.read.csv(\"abfss://bronze@storagekevinav.dfs.core.windows.net/sales_view/sales\",header=True,inferSchema=True).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5263e45a-487a-4afd-ad7d-70c25517e3c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CleanColumns\").getOrCreate()\n",
    "\n",
    "\n",
    "dfsales = spark.read.option(\"header\", True).csv(\"abfss://bronze@storagekevinav.dfs.core.windows.net/sales_view/sales\", inferSchema=True)\n",
    "\n",
    "\n",
    "def rename_columns_to_snake_case(dfsales):\n",
    "    def camel_to_snake(colname):\n",
    "        s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', colname)\n",
    "        colname = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "        return colname.lower().replace(\" \", \"_\")\n",
    "    return dfsales.toDF(*[camel_to_snake(c) for c in dfsales.columns])\n",
    "\n",
    "def clean_column_names(dfsales):\n",
    "    new_cols = []\n",
    "    for c in dfsales.columns:\n",
    "        clean = re.sub(r\"[ ,;{}()\\n\\t=]\", \"_\", c)\n",
    "        clean = re.sub(\"_+\", \"_\", clean)\n",
    "        new_cols.append(clean.lower())\n",
    "    return dfsales.toDF(*new_cols)\n",
    "\n",
    "# Step 3: Apply transformations\n",
    "df_sales = clean_column_names(rename_columns_to_snake_case(dfsales))\n",
    "\n",
    "# Step 4: Display\n",
    "display(df_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8414fc1-b9b1-4838-8bdc-df6e5c66e73c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Convert all date columns → yyyy-MM-dd\n",
    " \n",
    "#First detect date columns dynamically\n",
    "date_columns = [c for c in df_sales.columns if \"date\" in c]\n",
    "print(\"Date columns:\", date_columns)\n",
    " \n",
    "#Apply on that columns\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date\n",
    " \n",
    "for dc in date_columns:\n",
    "    df_datechange = (\n",
    "        df_sales\n",
    "        .withColumn(dc + \"_ts\", to_timestamp(col(dc), \"dd-MM-yyyy HH:mm\"))\n",
    "        .withColumn(dc + \"_date\", to_date(col(dc + \"_ts\")))\n",
    "        .drop(dc)        \n",
    "        .drop(dc + \"_ts\")  \n",
    "        .withColumnRenamed(dc + \"_date\", dc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45385cbe-ee6b-4a31-b05d-e23aef764694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_redup = df_datechange.dropDuplicates([\"order_id\", \"product_id\", \"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842c415c-9358-4dfd-af25-4bf993449584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    " \n",
    "table_name = \"silver_data.sales_view.sales\"\n",
    " \n",
    "if spark.catalog.tableExists(table_name):\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    " \n",
    "    (\n",
    "        delta_table.alias(\"target\")\n",
    "        .merge(\n",
    "            df_redup.alias(\"source\"),\n",
    "            \"target.order_id = source.order_id AND \"\n",
    "            \"target.product_id = source.product_id AND \"\n",
    "            \"target.customer_id = source.customer_id\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "else:\n",
    "    df_redup.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    " \n",
    "print(\"Sales data successfully upserted into Silver layer Catalog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d4c024-3d57-4ede-be53-c536fcf5a305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    " table_name = \"assignment_adf.sales_view.sales\"\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS sales USING delta LOCATION 'adfss://gold-transformation@storagekevinav.dfs.core.windows.net/sales'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98443bf7-84a7-413e-9037-8b7fc2e8edc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"silver_data.sales_view.customer\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://silver-transformation@storagekevinav.dfs.core.windows.net/customer\")\n",
    "DROP TABLE silver_data.sales_view.customer;\n",
    "CREATE TABLE silver_data.sales_view.customer\n",
    "USING DELTA\n",
    "LOCATION 'abfss://silver-transformation@storagekevinav.dfs.core.windows.net/customer';\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e87347a-8786-4b74-a41a-f469f0cdb032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = spark.table(\"silver_data.sales_view.product\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://silver-transformation@storagekevinav.dfs.core.windows.net/product\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a868b8-69c3-4328-8b47-e9298ebeaea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql DROP TABLE silver_data.sales_view.product;\n",
    "CREATE TABLE silver_data.sales_view.product\n",
    "USING DELTA\n",
    "LOCATION 'abfss://silver-transformation@storagekevinav.dfs.core.windows.net/product';\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175a0e03-17a1-4d03-9c4c-a05ca164c275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"silver_data.sales_view.store\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://silver-transformation@storagekevinav.dfs.core.windows.net/store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e57a4e-5be0-440c-90d2-98dc427c03ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql DROP TABLE silver_data.sales_view.store;\n",
    "CREATE TABLE silver_data.sales_view.store\n",
    "USING DELTA\n",
    "LOCATION 'abfss://silver-transformation@storagekevinav.dfs.core.windows.net/store';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0ba379-b599-4292-8d86-6b6064b1b06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"silver_data.sales_view.sales\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://silver-transformation@storagekevinav.dfs.core.windows.net/sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df0ebba7-cb33-433c-a54a-a6baf4245840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7015335484127811,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
